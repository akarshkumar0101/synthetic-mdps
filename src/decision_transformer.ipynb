{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba43845-aef1-4b0f-8bab-4a1d77b62216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oliva-titanrtx-2.csail.mit.edu\n",
      "/data/vision/phillipi/akumar01/synthetic-mdps/src\n",
      "/data/vision/phillipi/akumar01/.virtualenvs/dt/bin/python\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2  \n",
    "!hostname  \n",
    "!pwd  \n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ['LD_LIBRARY_PATH'] = '/data/vision/phillipi/akumar01/.mujoco/mujoco210/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa256f-05af-489a-9dd0-3b2494c61990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56afd25c-5b9b-45d2-8b0e-32c8c3d12795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, pickle\n",
    "from functools import partial  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140c4aa9-3a8e-4447-b6f0-df99800a6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc766be-662e-4635-ba0b-77baee5bd0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c08d9cd-e2e0-4d53-b529-08495d00d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_obs: int = 10\n",
    "    d_act: int = 10\n",
    "    block_size: int = 20\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 256\n",
    "\n",
    "    dropout: float = 0\n",
    "    bias: bool = True\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.embed_rtg = nn.Linear(1, config.n_embd)\n",
    "        self.embed_obs = nn.Linear(config.d_obs, config.n_embd)\n",
    "        self.embed_act = nn.Linear(config.d_act, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.d_act, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        # self.embed_act.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, rtg, obs, act):\n",
    "        device = rtg.device\n",
    "        B, T, _ = obs.shape\n",
    "        assert T <= self.config.block_size\n",
    "        \n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (t)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        pos_emb = self.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "\n",
    "        print(rtg.shape, obs.shape, act.shape)\n",
    "        print(self.embed_rtg.weight.shape)\n",
    "        print(self.embed_obs.weight.shape)\n",
    "        print(self.embed_act.weight.shape)\n",
    "        tok_rtg = pos_emb + self.embed_rtg(rtg) # token embeddings of shape (b, t, n_embd)\n",
    "        tok_obs = pos_emb + self.embed_obs(obs) # token embeddings of shape (b, t, n_embd)\n",
    "        tok_act = pos_emb + self.embed_act(act) # token embeddings of shape (b, t, n_embd)\n",
    "        print('bef', tok_rtg.shape)\n",
    "        x = rearrange([tok_rtg, tok_obs, tok_act], 'roa B T D -> B (T roa) D')\n",
    "        print('aft', x.shape)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        x = rearrange(x, 'B (T roa) D -> B T roa D', roa=3)\n",
    "        print('a', x.shape)\n",
    "        x = x[:, :, -1, :]\n",
    "        print('b', x.shape)\n",
    "        act_pred = self.lm_head(x)\n",
    "        return act_pred\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b63fb67-60c8-4e90-a020-93e1de3a0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4d7c25-92bf-45b8-9430-1a39e7a5b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.17M\n"
     ]
    }
   ],
   "source": [
    "config = Config(d_obs=32, d_act=8, block_size=20, n_layer=4, n_head=4, n_embd=256)\n",
    "model = DecisionTransformer(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d1ad63-c0d8-4e7a-8edb-6f0e8c89332c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1]) torch.Size([1, 20, 32]) torch.Size([1, 20, 8])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 32])\n",
      "torch.Size([256, 8])\n",
      "bef torch.Size([1, 20, 256])\n",
      "aft torch.Size([1, 60, 256])\n",
      "a torch.Size([1, 20, 3, 256])\n",
      "b torch.Size([1, 20, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0353e-01, -3.8569e-01,  3.9646e-01,  4.9481e-01,  6.7713e-01,\n",
       "          -4.6159e-01,  7.9908e-01,  1.6425e-02],\n",
       "         [-3.3877e-02,  1.4053e-01,  3.2075e-01,  3.9774e-01, -6.1029e-02,\n",
       "           3.1290e-01, -6.1294e-02, -2.7331e-01],\n",
       "         [-1.8243e-01,  1.6424e-01,  1.6112e-01, -3.1307e-01, -2.3705e-01,\n",
       "          -9.5056e-02,  2.3738e-01, -5.6429e-02],\n",
       "         [-1.9602e-01,  3.0718e-01,  1.3443e-03,  3.5842e-01, -3.1353e-01,\n",
       "           3.1940e-01, -6.2291e-02, -3.8801e-01],\n",
       "         [-6.5750e-02, -3.0030e-01, -1.2706e-02,  2.8188e-01, -1.2637e-01,\n",
       "          -3.8196e-01,  3.9602e-01,  1.3794e-01],\n",
       "         [ 9.0921e-01, -4.2611e-02, -1.4584e-01, -3.8703e-02, -3.9624e-01,\n",
       "           3.0166e-01,  9.3948e-02, -4.2687e-01],\n",
       "         [ 4.0376e-01, -8.7771e-02,  4.0322e-01, -2.7523e-01, -4.6761e-01,\n",
       "          -4.3732e-02,  5.8071e-01, -4.2634e-01],\n",
       "         [ 8.6821e-01,  4.8203e-01,  1.7311e-03, -7.8554e-02,  2.7821e-01,\n",
       "           6.6845e-01,  2.4356e-01, -8.4900e-02],\n",
       "         [ 3.4611e-01,  4.1917e-01,  2.5891e-01, -8.0533e-02, -4.3890e-01,\n",
       "           6.4633e-01, -8.4731e-01, -1.3309e-01],\n",
       "         [-5.7911e-01, -4.0468e-01,  6.2603e-01, -9.5449e-02, -2.7184e-01,\n",
       "           1.1658e-01,  1.8264e-01, -1.7864e-01],\n",
       "         [ 2.8849e-02, -2.5935e-01,  8.5806e-03,  3.0278e-01, -2.5434e-01,\n",
       "          -7.4857e-03, -1.2478e-01, -5.8070e-02],\n",
       "         [-1.5881e-01,  1.8270e-01,  1.4366e-01, -3.4097e-02, -5.6422e-01,\n",
       "           7.5924e-01, -1.8287e-01, -2.6482e-01],\n",
       "         [ 4.4590e-01, -3.7424e-01, -1.3787e-01,  5.7386e-01,  2.3553e-01,\n",
       "          -7.7044e-02,  2.0557e-01,  6.2077e-01],\n",
       "         [ 2.0407e-01,  2.7050e-01, -1.0788e-01,  6.6358e-01,  2.8132e-01,\n",
       "          -1.9308e-01, -1.8941e-01,  3.3206e-01],\n",
       "         [-4.5760e-02,  1.0181e-01,  1.4887e-01, -4.9093e-01,  5.6626e-01,\n",
       "          -1.9560e-01,  4.6226e-01, -2.5495e-01],\n",
       "         [ 9.9373e-02, -2.4709e-01,  2.0389e-01,  8.7548e-02,  3.9613e-01,\n",
       "          -3.6153e-01,  4.9376e-01,  1.8983e-01],\n",
       "         [ 4.9095e-01,  5.2379e-02,  2.3490e-01,  8.4085e-02, -4.7867e-04,\n",
       "          -9.0884e-02,  7.9589e-01, -3.2840e-01],\n",
       "         [ 4.9566e-01,  4.2262e-01, -2.3027e-01, -5.9983e-01,  1.5631e-02,\n",
       "           4.2628e-01,  5.9819e-02, -2.8835e-01],\n",
       "         [ 4.3279e-01,  3.0873e-02, -2.8256e-02,  2.9708e-01, -1.8571e-01,\n",
       "          -5.0273e-01,  3.2623e-01,  3.8523e-01],\n",
       "         [ 3.9721e-01,  1.7983e-01,  3.1968e-02,  3.3187e-01,  1.7996e-01,\n",
       "           1.5179e-01,  2.3400e-01,  1.2860e-01]]], device='cuda:0',\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 20, 1).to(device), torch.randn(1, 20, 32).to(device), torch.randn(1, 20, 8).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed184999-3b77-4162-a1cf-2a9a12dc654b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6612310d-9278-440e-bf5a-97555391cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/builder.py:9: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead\n",
      "  from distutils.sysconfig import customize_compiler\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error. Trying to rebuild mujoco_py.\n",
      "Compiling /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.pyx because it changed.\n",
      "[1/1] Cythonizing /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.pyx\n",
      "running build_ext\n",
      "building 'mujoco_py.cymj' extension\n",
      "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py -I/data/vision/phillipi/akumar01/.mujoco/mujoco210/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/numpy/core/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/include -I/usr/include/python3.10 -c /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.c -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.o -fopenmp -w\n",
      "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py -I/data/vision/phillipi/akumar01/.mujoco/mujoco210/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/numpy/core/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/include -I/usr/include/python3.10 -c /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.c -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
      "x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -L/data/vision/phillipi/akumar01/.mujoco/mujoco210/bin -Wl,--enable-new-dtags,-R/data/vision/phillipi/akumar01/.mujoco/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/lib.linux-x86_64-3.10/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Warning: Mujoco-based envs failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "libglewosmesa.so: cannot open shared object file: No such file or directory\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error. Trying to rebuild mujoco_py.\n",
      "running build_ext\n",
      "building 'mujoco_py.cymj' extension\n",
      "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py -I/data/vision/phillipi/akumar01/.mujoco/mujoco210/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/numpy/core/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/include -I/usr/include/python3.10 -c /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.c -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.o -fopenmp -w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py -I/data/vision/phillipi/akumar01/.mujoco/mujoco210/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/numpy/core/include -I/data/vision/phillipi/akumar01/.virtualenvs/dt/include -I/usr/include/python3.10 -c /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.c -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
      "x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/cymj.o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -L/data/vision/phillipi/akumar01/.mujoco/mujoco210/bin -Wl,--enable-new-dtags,-R/data/vision/phillipi/akumar01/.mujoco/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /data/vision/phillipi/akumar01/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/lib.linux-x86_64-3.10/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "libglewosmesa.so: cannot open shared object file: No such file or directory. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m const\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m \u001b[43mload_cython_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/builder.py:111\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m         cext_so_path \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m--> 111\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mload_dynamic_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcymj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcext_so_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/mujoco_py/builder.py:130\u001b[0m, in \u001b[0;36mload_dynamic_ext\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m loader \u001b[38;5;241m=\u001b[39m ExtensionFileLoader(name, path)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mImportError\u001b[0m: libglewosmesa.so: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\n",
      "File \u001b[0;32m~/d4rl/d4rl/__init__.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgym_bullet\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpointmaze_bullet\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SUPPRESS_MESSAGES:\n",
      "File \u001b[0;32m~/d4rl/d4rl/pointmaze_bullet/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpointmaze\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaze_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OPEN, U_MAZE, MEDIUM_MAZE, LARGE_MAZE, U_MAZE_EVAL, MEDIUM_MAZE_EVAL, LARGE_MAZE_EVAL\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infos\n",
      "File \u001b[0;32m~/d4rl/d4rl/pointmaze/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaze_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MazeEnv, OPEN, U_MAZE, MEDIUM_MAZE, LARGE_MAZE, U_MAZE_EVAL, MEDIUM_MAZE_EVAL, LARGE_MAZE_EVAL\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[1;32m      4\u001b[0m register(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaze2d-open-v0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md4rl.pointmaze:MazeEnv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/d4rl/d4rl/pointmaze/maze_model.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" A pointmass maze env.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mujoco_env\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m offline_env\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/gym/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MujocoEnv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ^^^^^ so that user gets the correct error\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# message if mujoco is not installed correctly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AntEnv\n",
      "File \u001b[0;32m~/.virtualenvs/dt/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     16\u001b[0m             e\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m DEFAULT_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_observation_to_space\u001b[39m(observation):\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: libglewosmesa.so: cannot open shared object file: No such file or directory. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1f5d2-4eee-48ba-8236-15d21134c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73faa84c-693b-4ea2-a0c4-8fc2cb12fc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398eebbd-fd24-473e-b2fc-3014c77cf806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93b784-eca6-47b0-9931-2221974c22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afbc7d-fb39-4f5f-8fdc-9c202df0cdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c7f53-671e-4135-8ed0-28f1c2d10dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce34ec-819b-47d0-929c-139b500996c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2f0a8-3a30-4b1c-9420-8571f08a4121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f11937-e57f-4617-9bef-bf3bd5bdcd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
